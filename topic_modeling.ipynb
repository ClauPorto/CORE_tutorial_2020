{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Topic modeling\n",
    "Another way to compare documents is to extract the latent topics that group words within each document, and compare those distributions.\n",
    "\n",
    "We'll continue on the topic of fake news with another dataset that has examples of both fake and real news articles, at a much larger scale than the previous data.\n",
    "\n",
    "1. [Load data](#Load-data)\n",
    "2. [Latent Semantic Analysis](#Latent-Semantic-Analysis)\n",
    "3. [Latent Dirichlet Allocation](#Latent-Dirichlet-Allocation)\n",
    "4. [Exploration](#Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data = fake news challenge\n",
    "import pandas as pd\n",
    "fake_news_article_data = pd.read_csv('data/fake_news_challenge/Fake.csv', sep=',', index_col=False)\n",
    "real_news_article_data = pd.read_csv('data/fake_news_challenge/True.csv', sep=',', index_col=False)\n",
    "# get rid of duplicate articles\n",
    "fake_news_article_data.drop_duplicates('text', inplace=True)\n",
    "real_news_article_data.drop_duplicates('text', inplace=True)\n",
    "display(fake_news_article_data.loc[:, 'text'].head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try topic modeling, we have to convert the text to a usable format (document-term matrix, like before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from stop_words import get_stop_words\n",
    "## combine text data, keep track of fake/real news indices\n",
    "combined_news_text = fake_news_article_data.loc[:, 'text'].append(real_news_article_data.loc[:, 'text'])\n",
    "fake_news_text_indices = list(range(fake_news_article_data.shape[0]))\n",
    "real_news_text_indices = list(range(fake_news_article_data.shape[0], combined_news_text.shape[0]))\n",
    "## convert text to DTM\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, lowercase=True, \n",
    "                     ngram_range=(1,1), stop_words=en_stops, tokenizer=tokenizer.tokenize)\n",
    "combined_news_text_dtm = cv.fit_transform(combined_news_text)\n",
    "print(combined_news_text_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first method, let's try Latent Semantic Analysis, which is a form of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "num_topics = 10\n",
    "num_iter = 10\n",
    "lsa_model = TruncatedSVD(n_components=num_topics, n_iter=num_iter, random_state=123)\n",
    "combined_news_text_lsa_topics = lsa_model.fit_transform(combined_news_text_dtm)\n",
    "print(combined_news_text_lsa_topics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSA process outputs continuous values [-inf, +inf] which we need to convert to probabilities [0,1]. We can use the softmax function along each dimension to convert the topic-document matrix to probabilities:\n",
    "\n",
    "$$\\text{softmax}(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{j}^{K}e^{x_{j}}}$$\n",
    "\n",
    "where $x$ is one of $K$ topic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# convert per-column scores to a normal distribution (0,1)\n",
    "scaler = StandardScaler()\n",
    "combined_news_text_lsa_topic_scores = scaler.fit_transform(combined_news_text_lsa_topics)\n",
    "# soft-max per-column\n",
    "combined_news_text_lsa_topic_probs = softmax(combined_news_text_lsa_topic_scores.T).T\n",
    "# normalize per-row so that probabilities sum to 1\n",
    "combined_news_text_lsa_topic_probs = combined_news_text_lsa_topic_probs / combined_news_text_lsa_topic_probs.sum(axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected probability of a document being assigned to a topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_news_text_lsa_expected_topics = pd.Series(combined_news_text_lsa_topic_probs.mean(axis=0))\n",
    "print(f'expected probability of topics = \\n{combined_news_text_lsa_expected_topics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data is \"dominated\" by 3 topics with high probability.\n",
    "\n",
    "To figure out what \"topics\" the model learned, let's look at the news articles with the highest probability for each topic.\n",
    "\n",
    "We'll take the arg-max along each topic and print the text for the corresponding articles.\n",
    "We'll look at the most likely topics (0, 4, 7) as a first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_articles_with_highest_prob_per_topic(doc_topic_probs, doc_text, num_topics):\n",
    "    topic_ids = list(range(num_topics))\n",
    "    top_articles_per_topic = 10\n",
    "    text_sample_len = 200\n",
    "    for topic_id_i in topic_ids:\n",
    "        print(f'processing topic {topic_id_i}')\n",
    "        # get indices for articles with highest topic probability\n",
    "        top_article_indices_i = np.argsort(doc_topic_probs[:, topic_id_i])[-top_articles_per_topic:]\n",
    "        top_article_indices_i = list(reversed(top_article_indices_i))\n",
    "        for index_j in top_article_indices_i:\n",
    "            topic_prob_i_j = doc_topic_probs[index_j, topic_id_i]\n",
    "            print(f'\\tarticle {index_j} has P(topic)={topic_prob_i_j} with text = {doc_text.iloc[index_j][:text_sample_len]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_articles_with_highest_prob_per_topic(combined_news_text_lsa_topic_probs, combined_news_text, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = list(range(num_topics))\n",
    "top_articles_per_topic = 10\n",
    "for topic_id_i in topic_ids:\n",
    "    print(f'processing topic {topic_id_i}')\n",
    "    # get indices for articles with highest topic probability\n",
    "    top_article_indices_i = np.argsort(combined_news_text_lsa_topic_probs[:, topic_id_i])[-top_articles_per_topic:]\n",
    "    top_article_indices_i = list(reversed(top_article_indices_i))\n",
    "    for index_j in top_article_indices_i:\n",
    "        topic_prob_i_j = combined_news_text_lsa_topic_probs[index_j, topic_id_i]\n",
    "        print(f'\\tarticle {index_j} has P(topic)={topic_prob_i_j} with text = {combined_news_text.iloc[index_j][:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the article text qualitatively, we observe the following:\n",
    "\n",
    "- Topic 0 includes major election issues such as U.S. president Trump's campaign and action in office.\n",
    "- Topic 4 includes more subjective claims (`anti-American`, `whine`) and more extreme issues (`conspiracy`, `chaos`, `violence`).\n",
    "- Topic 7 includes discussion of the 2016 election, particularly related to Clinton (`email`, `classified`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which topics are more prevalent in fake news versus real news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_text_lsa_topic_probs = combined_news_text_lsa_topic_probs[fake_news_text_indices, :]\n",
    "real_news_text_lsa_topic_probs = combined_news_text_lsa_topic_probs[real_news_text_indices, :]\n",
    "fake_news_text_lsa_expected_topics = pd.Series(fake_news_text_lsa_topic_probs.mean(axis=0))\n",
    "real_news_text_lsa_expected_topics = pd.Series(real_news_text_lsa_topic_probs.mean(axis=0))\n",
    "print(f'expected probability of topics for fake news = \\n{fake_news_text_lsa_expected_topics}')\n",
    "print(f'expected probability of topics for real news = \\n{real_news_text_lsa_expected_topics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like real news discusses topic 0 (possible criticism of Trump?) slightly more than fake news, while fake news discusses discusses topic 4 (conspiracy theories?) slightly more than real news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a useful first pass on the data, it doesn't help us identify which words or phrases may differentiate fake news from real news. \n",
    "\n",
    "We'll move onto a more complicated method (Latent Dirichlet Allocation) that identifies latent topics from which words are \"generated.\" \n",
    "This will help us pull out specific words that characterize the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA\n",
    "# get text tokens first using the CountVectorizer from earlier\n",
    "combined_news_text_dtm_tokens = cv.inverse_transform(combined_news_text_dtm)\n",
    "from gensim.corpora import Dictionary\n",
    "lda_dict = Dictionary(combined_news_text_dtm_tokens)\n",
    "combined_news_text_corpus = list(map(lambda x: lda_dict.doc2bow(x), combined_news_text_dtm_tokens))\n",
    "# train model\n",
    "from gensim.models import LdaModel\n",
    "num_topics = 10\n",
    "iterations = 50\n",
    "lda_model = LdaModel(corpus=combined_news_text_corpus, num_topics=10, iterations=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, let's look at the distribution of topics over all documents and get a sense of the articles that correspond to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lda_topic_probs(text_doc, model):\n",
    "    doc_topics = model.get_document_topics(text_doc, minimum_probability=0.)\n",
    "    # convert to probability array\n",
    "    doc_topic_ids, doc_topic_probs = zip(*doc_topics)\n",
    "    return doc_topic_probs\n",
    "combined_news_text_lda_topic_probs = np.array(list(map(lambda x: compute_lda_topic_probs(x, lda_model), combined_news_text_corpus)))\n",
    "combined_news_text_lda_topic_expected_prob = combined_news_text_lda_topic_probs.mean(axis=0)\n",
    "print(f'expected value of LDA topics =\\n{combined_news_text_lda_topic_expected_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the SVD analysis, we see a more even distribution of topics. Let's see which articles were more strongly associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_articles_with_highest_prob_per_topic(combined_news_text_lda_topic_probs, combined_news_text, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricting ourselves to the top 5 most frequent topics in the data based on the probabilities above (topics 3, 8, 9, 1, 2), we see the following trends:\n",
    "\n",
    "- Topic 1 includes U.S. election issues and general content concerning the president.\n",
    "- Topic 2 includes disasters and violence, possibly fear-mongering.\n",
    "- Topic 3 includes international politics.\n",
    "- Topic 8 seems to include inflammatory and \"alternative\" news content (`hypocrites`, `trashing`).\n",
    "- Topic 9 includes the politics around U.S. healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compare the distribution of topics in each text category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_text_lda_topic_probs = combined_news_text_lda_topic_probs[fake_news_text_indices, :]\n",
    "real_news_text_lda_topic_probs = combined_news_text_lda_topic_probs[real_news_text_indices, :]\n",
    "fake_news_text_lda_expected_topics = pd.Series(fake_news_text_lda_topic_probs.mean(axis=0))\n",
    "real_news_text_lda_expected_topics = pd.Series(real_news_text_lda_topic_probs.mean(axis=0))\n",
    "print(f'expected probability of topics for fake news = \\n{fake_news_text_lda_expected_topics}')\n",
    "print(f'expected probability of topics for real news = \\n{real_news_text_lda_expected_topics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real news articles tend to have more representation for topics 3 and 9, while fake news articles have more representation for topics 1, 2 and 8, which makes sense given the more violent and \"alternative\" content included in those topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've established the high-level differences in topics between fake news and real news, let's look at the individual words that make up the topics.\n",
    "\n",
    "Specifically, we're going to compute the probability of observing a word given a topic, using the parameters learned by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_words_all_topics(model, model_dict, num_topics, words_per_topic):\n",
    "    topic_ids = list(range(num_topics))\n",
    "    for topic_i in topic_ids:\n",
    "        topic_word_id_scores_i = model.get_topic_terms(topic_i, topn=words_per_topic)\n",
    "        topic_word_ids_i, topic_word_scores_i = zip(*topic_word_id_scores_i)\n",
    "        # convert word ID to words\n",
    "        topic_words_i = list(map(model_dict.get, topic_word_ids_i))\n",
    "        print(f'topic {topic_i} has top words: \\n\\t{\", \".join(topic_words_i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_topic = 20\n",
    "show_top_words_all_topics(lda_model, lda_dict, num_topics, words_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top words confirms what we saw before, that fake news articles tend to focus on election conflicts (topic 1), violence (topic 2), and possibly more simple or engaging words to correspond with more \"opinion\" pieces (topic 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we train separate topic models on real news and fake news? This could help highlight groups of words that are specific only to fake news or to real news, which may be \"washed out\" with the combined topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "iterations = 100\n",
    "# train fake news model\n",
    "def train_lda_model_from_corpus(text_corpus, num_topics, iterations):\n",
    "    lda_model = LdaModel(text_corpus, num_topics=num_topics, iterations=iterations)\n",
    "    return lda_model\n",
    "# fake_news_text_dtm_tokens = list(map(lambda x: combined_news_text_dtm_tokens[x], fake_news_text_indices))\n",
    "# real_news_text_dtm_tokens = list(map(lambda x: combined_news_text_dtm_tokens[x], real_news_text_indices))\n",
    "fake_news_text_corpus = list(map(lambda x: combined_news_text_corpus[x], fake_news_text_indices))\n",
    "real_news_text_corpus = list(map(lambda x: combined_news_text_corpus[x], real_news_text_indices))\n",
    "## train models\n",
    "fake_news_lda_model = train_lda_model_from_corpus(fake_news_text_corpus, num_topics, iterations)\n",
    "real_news_lda_model = train_lda_model_from_corpus(real_news_text_corpus, num_topics, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top words captured per-topic from each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_topic = 20\n",
    "print('real news: top words per topic')\n",
    "show_top_words_all_topics(real_news_lda_model, lda_dict, num_topics, words_per_topic)\n",
    "print('fake news: top words per topic')\n",
    "show_top_words_all_topics(fake_news_lda_model, lda_dict, num_topics, words_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real news topics include concrete details and \"normal\" news items such as money (topic 1), immigration statistics (topic 5), and international diplomacy (topic 7).\n",
    "\n",
    "The fake news topics include sub-discussions around Donald Trump (topic 2: Trump vs. Obama; topic 5: election results) and some topics related to social justice (topic 7: `black`, `white`; topic 8: `protesters`, `police`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Now it's time for you to keep exploring what topic models can tell us about real and fake news.\n",
    "\n",
    "Some ideas:\n",
    "- We used word frequency to represent words when training the topic models, but you can try other metrics such as TF-IDF, which we saw before can up-weight rarer words. What happens if you re-train the topic model using another form of word frequency?\n",
    "- You can change the number of topics learned by the model to include more or less detail that may reveal different \"levels\" of granularity. You may want to try using \"coherence\" as a metric to determine the number of topics that maximizes the similarity among words within the same topic. What broad or fine-grained differences can you find that differentiate real and fake news? \n",
    "- One way of reducing \"overlap\" among words within topics is to **stem** each word and convert it to a base form that is shared among different versions of the word (e.g. `dog` and `dogs` stemmed to `dog`). What happens if you stem the text before training the topic model?\n",
    "- Some topics may be closer together in \"space\" than others. For instance, topics that discuss different aspects of international relations. [This package](https://github.com/bmabey/pyLDAvis) visualizes the relationship between LDA topics by projecting the topics to a shared 2-dimensional space via PCA. Can you find topics that are unexpectedly close, and whether these topics indicate similarities or differences between real and fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CORE_tutorial] *",
   "language": "python",
   "name": "conda-env-CORE_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
