{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:CORE_tutorial] *",
      "language": "python",
      "name": "conda-env-CORE_tutorial-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "word_frequency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dATkW4o401nV"
      },
      "source": [
        "# Word frequency\n",
        "\n",
        "First step in exploration: which words occur more frequently in one data set versus another?\n",
        "\n",
        "1. [Load data](#Load-data)\n",
        "2. [Raw frequency](#Raw-frequency)\n",
        "3. [Normalized frequency](#Normalized-frequency)\n",
        "4. [TF-IDF](#TF-IDF)\n",
        "5. [Exploration](#Exploration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p65y7syS01ni"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtXUf8s2AKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4860a86b-0bcd-4d6d-cc24-dd6983528c82"
      },
      "source": [
        "!wget https://bitbucket.org/istewart6/core_tutorial_2020/raw/36e69f9d777319ae2cc94354cf57bd01f3e080b3/data.zip; unzip data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 22:31:43--  https://bitbucket.org/istewart6/core_tutorial_2020/raw/36e69f9d777319ae2cc94354cf57bd01f3e080b3/data.zip\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c3:9b0a, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 230909807 (220M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 220.21M   108MB/s    in 2.0s    \n",
            "\n",
            "2020-12-09 22:31:51 (108 MB/s) - ‘data.zip’ saved [230909807/230909807]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/fakeNewsDatasets/\n",
            "  inflating: data/fakeNewsDatasets/fake_news_small.tsv  \n",
            "  inflating: data/fakeNewsDatasets/real_news_small.tsv  \n",
            "   creating: data/fake_news_challenge/\n",
            "  inflating: data/fake_news_challenge/fake_news_glove_embed.model  \n",
            "  inflating: data/fake_news_challenge/real_news_word2vec_embed.model  \n",
            "  inflating: data/fake_news_challenge/Fake.csv  \n",
            "  inflating: data/fake_news_challenge/fake_news_word2vec_embed.model  \n",
            "  inflating: data/fake_news_challenge/True.csv  \n",
            "  inflating: data/fake_news_challenge/real_news_glove_embed.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwkQeuTlorAS",
        "outputId": "54c44cea-3b11-4897-8a4e-ea6849909f17"
      },
      "source": [
        "!pip install stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=ae954665226e0323c0f7897db983bf43a1510e49f63e6738c6842e09629c3769\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xacSZvJs01nl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "afad4447-2a6b-4459-fbff-99d8be7fd144"
      },
      "source": [
        "import pandas as pd\n",
        "fake_news_data = pd.read_csv('data/fakeNewsDatasets/fake_news_small.tsv', sep='\\t', index_col=False)\n",
        "real_news_data = pd.read_csv('data/fakeNewsDatasets/real_news_small.tsv', sep='\\t', index_col=False)\n",
        "display(fake_news_data.head())\n",
        "display(real_news_data.head())\n",
        "print(fake_news_data.shape[0])\n",
        "# print(fake_news_data[5])\n",
        "# print(real_news_data[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alex Jones Vindicated in \"Pizzagate\" Controversy</td>\n",
              "      <td>\"Alex Jones, purveyor of the independent inves...</td>\n",
              "      <td>biz</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>THE BIG DATA CONSPIRACY</td>\n",
              "      <td>so that in the no so far future can institute ...</td>\n",
              "      <td>biz</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>California Surprisingly Lenient on Auto Emissi...</td>\n",
              "      <td>Setting Up Face-Off With Trump \"California's c...</td>\n",
              "      <td>biz</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mexicans Are Chomping at the Bit to Stop NAFTA...</td>\n",
              "      <td>Mexico has been unfairly gaining from NAFTA as...</td>\n",
              "      <td>biz</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Breaking News: Snapchat to purchase Twitter fo...</td>\n",
              "      <td>Yahoo and AOL could be extremely popular over ...</td>\n",
              "      <td>biz</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... id\n",
              "0   Alex Jones Vindicated in \"Pizzagate\" Controversy  ...  1\n",
              "1                            THE BIG DATA CONSPIRACY  ...  2\n",
              "2  California Surprisingly Lenient on Auto Emissi...  ...  3\n",
              "3  Mexicans Are Chomping at the Bit to Stop NAFTA...  ...  4\n",
              "4  Breaking News: Snapchat to purchase Twitter fo...  ...  5\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alex Jones Apologizes for Promoting 'Pizzagate...</td>\n",
              "      <td>Alex Jones  a prominent conspiracy theorist an...</td>\n",
              "      <td>biz</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Banks and Tech Firms Battle Over Something Aki...</td>\n",
              "      <td>The big banks and Silicon Valley are waging an...</td>\n",
              "      <td>biz</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>California Upholds Auto Emissions Standards</td>\n",
              "      <td>Setting Up Face-Off With Trump  \"California's ...</td>\n",
              "      <td>biz</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Renegotiate Nafta? Mexicans Say Get On With It</td>\n",
              "      <td>For more than two decades  free trade has been...</td>\n",
              "      <td>biz</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Snapchat 'will be bigger than Twitter</td>\n",
              "      <td>Yahoo and AOL with advertisers'  \"Snapchat cou...</td>\n",
              "      <td>biz</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... id\n",
              "0  Alex Jones Apologizes for Promoting 'Pizzagate...  ...  1\n",
              "1  Banks and Tech Firms Battle Over Something Aki...  ...  2\n",
              "2        California Upholds Auto Emissions Standards  ...  3\n",
              "3     Renegotiate Nafta? Mexicans Say Get On With It  ...  4\n",
              "4              Snapchat 'will be bigger than Twitter  ...  5\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_YWaY-01nr"
      },
      "source": [
        "### Raw frequency\n",
        "\n",
        "As a first step, let's look at raw word frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6vBU7uE01nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "647e4baa-785b-46d0-9dba-59867c0c8f0d"
      },
      "source": [
        "# word frequency\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from stop_words import get_stop_words\n",
        "en_stops = get_stop_words('en')\n",
        "tokenizer = WordPunctTokenizer()\n",
        "cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
        "                     tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
        "                     ngram_range=(1,1))\n",
        "# get vocab for all data\n",
        "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
        "combined_txt_dtm = cv.fit_transform(combined_txt)\n",
        "sorted_vocab = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
        "# get separate DTM for each news data\n",
        "cv = CountVectorizer(min_df=0.001, max_df=0.75, tokenizer=tokenizer.tokenize, stop_words=en_stops, vocabulary=sorted_vocab)\n",
        "fake_news_dtm = cv.fit_transform(fake_news_data.loc[:, 'text'].values)\n",
        "real_news_dtm = cv.fit_transform(real_news_data.loc[:, 'text'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Ti2a1T1d01nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49951aa-64c5-422e-daa0-5b56cb3693d7"
      },
      "source": [
        "## top words\n",
        "import numpy as np\n",
        "fake_news_dtm_top_words = pd.Series(np.array(fake_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
        "real_news_dtm_top_words = pd.Series(np.array(real_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
        "print(fake_news_dtm_top_words.head(20))\n",
        "print(real_news_dtm_top_words.head(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",            851\n",
            "'            384\n",
            "\"            343\n",
            "s            278\n",
            "-            182\n",
            "will         161\n",
            "trump        113\n",
            "said          90\n",
            ".\"            89\n",
            "new           86\n",
            "president     76\n",
            "t             61\n",
            "time          56\n",
            "can           54\n",
            "school        53\n",
            "one           53\n",
            "now           49\n",
            "many          48\n",
            "just          43\n",
            "years         43\n",
            "dtype: int64\n",
            "'        451\n",
            "-        344\n",
            "\"        339\n",
            "s        323\n",
            ",        314\n",
            "said     148\n",
            "will      87\n",
            "year      64\n",
            "also      53\n",
            "t         52\n",
            ".\"        51\n",
            "new       47\n",
            "trump     47\n",
            "$         45\n",
            "first     41\n",
            "one       41\n",
            "last      38\n",
            "(         38\n",
            "two       38\n",
            ":         37\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYxpeRg401ny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "794f55f4-013a-48d3-b4ca-03244e7fa71d"
      },
      "source": [
        "# per-topic\n",
        "article_topics = fake_news_data.loc[:, 'topic'].unique()\n",
        "en_stops = get_stop_words('en')\n",
        "tokenizer = WordPunctTokenizer()\n",
        "top_k = 20\n",
        "for topic_i in article_topics:\n",
        "    print(f'topic = {topic_i}')\n",
        "    fake_news_data_i = fake_news_data[fake_news_data.loc[:, 'topic']==topic_i]\n",
        "    real_news_data_i = real_news_data[real_news_data.loc[:, 'topic']==topic_i]\n",
        "    # get vocab, compute counts, etc.\n",
        "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
        "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
        "                         ngram_range=(1,1))\n",
        "    combined_txt_i = fake_news_data_i.loc[:, 'text'].append(real_news_data_i.loc[:, 'text'])\n",
        "    combined_txt_dtm_i = cv.fit_transform(combined_txt_i)\n",
        "    sorted_vocab_i = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
        "    # get separate DTM for each news data\n",
        "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
        "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
        "                         ngram_range=(1,1), vocabulary=sorted_vocab_i)\n",
        "    fake_news_dtm_i = cv.fit_transform(fake_news_data_i.loc[:, 'text'].values)\n",
        "    real_news_dtm_i = cv.fit_transform(real_news_data_i.loc[:, 'text'].values)\n",
        "    # get top counts\n",
        "    fake_news_dtm_top_words_i = pd.Series(np.array(fake_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
        "    real_news_dtm_top_words_i = pd.Series(np.array(real_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
        "    print('top words for fake news articles')\n",
        "    display(fake_news_dtm_top_words_i)\n",
        "    print('top words for real news articles')\n",
        "    display(real_news_dtm_top_words_i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "topic = biz\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ",            106\n",
              "'             41\n",
              "\"             34\n",
              "s             33\n",
              "-             28\n",
              "will          28\n",
              "uk            21\n",
              "said          19\n",
              "$             14\n",
              "trump         13\n",
              "eu            13\n",
              "deal          13\n",
              ".\"            13\n",
              "company       11\n",
              "many          10\n",
              "companies     10\n",
              "european      10\n",
              "now            9\n",
              "may            8\n",
              "jobs           8\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'            73\n",
              "s            65\n",
              "-            58\n",
              "\"            50\n",
              "said         44\n",
              "$            26\n",
              "will         18\n",
              "us           16\n",
              "1            16\n",
              ")            15\n",
              "company      15\n",
              ":            13\n",
              "last         13\n",
              "firm         13\n",
              "trump        13\n",
              "uk           13\n",
              "financial    13\n",
              "european     13\n",
              "eu           13\n",
              "two          12\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "topic = edu\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"            53\n",
              "school       47\n",
              "'            45\n",
              "students     38\n",
              "s            31\n",
              "-            23\n",
              "will         22\n",
              "education    20\n",
              "trump        15\n",
              "president    12\n",
              "new          12\n",
              "children     11\n",
              "student      10\n",
              ".\"           10\n",
              "said         10\n",
              "parents      10\n",
              "law          10\n",
              "time         10\n",
              "schools      10\n",
              "first        10\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'             29\n",
              "s             24\n",
              "-             24\n",
              "school        23\n",
              "students      21\n",
              "\"             19\n",
              "education     11\n",
              "said           9\n",
              ",\"             9\n",
              "student        8\n",
              "year           8\n",
              "percent        7\n",
              "children       6\n",
              "president      6\n",
              "according      5\n",
              "will           5\n",
              "college        5\n",
              ")              5\n",
              "(              5\n",
              "university     5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "topic = entmt\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ",            161\n",
              "\"            106\n",
              "s             64\n",
              "-             31\n",
              "will          29\n",
              ".\"            25\n",
              "t             24\n",
              "one           17\n",
              "time          16\n",
              "show          16\n",
              "new           16\n",
              "also          14\n",
              "said          14\n",
              "fans          13\n",
              "way           12\n",
              "now           11\n",
              "last          11\n",
              "just          11\n",
              "(             11\n",
              "character     10\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"        151\n",
              "s        102\n",
              "-         89\n",
              "said      42\n",
              ",         30\n",
              ".\"        29\n",
              "also      23\n",
              "t         21\n",
              "will      20\n",
              "one       16\n",
              "film      16\n",
              "year      16\n",
              "first     15\n",
              "told      14\n",
              "new       14\n",
              "--        14\n",
              "news      13\n",
              "show      13\n",
              "john      11\n",
              "years     11\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "topic = polit\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "trump         79\n",
              "'             69\n",
              "\"             59\n",
              "s             56\n",
              "president     50\n",
              "clinton       29\n",
              "-             25\n",
              "donald        22\n",
              "said          20\n",
              "house         16\n",
              "white         16\n",
              "washington    15\n",
              "will          14\n",
              ".\"            13\n",
              "just          11\n",
              "cnn           11\n",
              ")             11\n",
              "(             11\n",
              "obama         11\n",
              "us            11\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"            33\n",
              "'            32\n",
              "s            28\n",
              "trump        25\n",
              "-            20\n",
              "said         18\n",
              ",\"           12\n",
              "president    10\n",
              "mr            9\n",
              "clinton       7\n",
              "campaign      6\n",
              "t             5\n",
              "will          5\n",
              "time          5\n",
              ":             5\n",
              "first         5\n",
              "u             4\n",
              "america       4\n",
              "press         4\n",
              "order         4\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "topic = sports\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ",         148\n",
              "\"          55\n",
              "s          51\n",
              "-          41\n",
              "will       26\n",
              "game       24\n",
              "team       23\n",
              ".\"         21\n",
              "said       18\n",
              "two        16\n",
              "one        14\n",
              "years      13\n",
              "year       13\n",
              "last       12\n",
              "time       11\n",
              "new        10\n",
              "brazil     10\n",
              "world      10\n",
              "sports      9\n",
              "just        9\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-          124\n",
              "s           83\n",
              "\"           71\n",
              "will        25\n",
              "year        23\n",
              "said        21\n",
              "world       17\n",
              "game        16\n",
              "sport       16\n",
              "one         15\n",
              ".\"          15\n",
              "two         14\n",
              "win         14\n",
              "time        13\n",
              "6           13\n",
              "team        13\n",
              "federer     12\n",
              "old         12\n",
              "sports      11\n",
              "(           11\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "topic = tech\n",
            "top words for fake news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'           49\n",
              "s           43\n",
              "will        42\n",
              "\"           36\n",
              "new         34\n",
              "-           34\n",
              "can         16\n",
              "amazon      14\n",
              "google      13\n",
              "now         12\n",
              "many        11\n",
              "apple       10\n",
              "t            9\n",
              "world        9\n",
              "devices      9\n",
              "said         9\n",
              "(            9\n",
              "time         9\n",
              "research     8\n",
              "app          8\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top words for real news articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-             29\n",
              "'             27\n",
              "s             21\n",
              "\"             15\n",
              "will          14\n",
              "said          14\n",
              "new           12\n",
              ",\"             9\n",
              "also           7\n",
              "devices        7\n",
              "can            6\n",
              "google         6\n",
              "year           6\n",
              "like           6\n",
              "t              6\n",
              "announced      5\n",
              "monday         5\n",
              "see            5\n",
              "game           4\n",
              "technology     4\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPCPoXe01n4"
      },
      "source": [
        "def compute_frequency(text_data, tokenizer, stops, vocab):\n",
        "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
        "                         ngram_range=(1,1), vocabulary=vocab)\n",
        "    dtm = cv.fit_transform(text_data)\n",
        "    word_frequency = np.array(dtm.sum(axis=0))[0]\n",
        "    word_frequency = pd.Series(word_frequency, index=vocab)\n",
        "    return word_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MRJUbRnG01n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d843ea9a-824e-4ee0-f8ea-8cdd8b14610a"
      },
      "source": [
        "fake_news_text = fake_news_data.loc[:, 'text'].values\n",
        "real_news_text = real_news_data.loc[:, 'text'].values\n",
        "fake_news_word_frequency = compute_frequency(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
        "real_news_word_frequency = compute_frequency(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
        "# compute difference\n",
        "fake_vs_real_news_word_frequency_diff = fake_news_word_frequency - real_news_word_frequency\n",
        "fake_vs_real_news_word_frequency_diff.sort_values(inplace=True, ascending=False)\n",
        "# show words with highest/lowest difference\n",
        "top_k = 20\n",
        "print('words that occurred in more fake news articles')\n",
        "print(fake_vs_real_news_word_frequency_diff.head(top_k))\n",
        "print('words that occurred in more real news articles')\n",
        "print(fake_vs_real_news_word_frequency_diff.tail(top_k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words that occurred in more fake news articles\n",
            ",            537\n",
            "will          74\n",
            "trump         66\n",
            "president     41\n",
            "new           39\n",
            ".\"            38\n",
            "many          33\n",
            "clinton       27\n",
            "donald        25\n",
            "time          24\n",
            "now           24\n",
            "school        24\n",
            "can           23\n",
            "stated        20\n",
            "students      19\n",
            "even          18\n",
            "white         18\n",
            "order         16\n",
            "way           16\n",
            "great         16\n",
            "dtype: int64\n",
            "words that occurred in more real news articles\n",
            ")             -8\n",
            "000           -9\n",
            "–             -9\n",
            "6            -10\n",
            "report       -11\n",
            "m            -11\n",
            "4            -11\n",
            "tuesday      -11\n",
            "three        -11\n",
            "1            -12\n",
            "financial    -12\n",
            "$            -16\n",
            "--           -17\n",
            "also         -19\n",
            ":            -20\n",
            "year         -27\n",
            "s            -45\n",
            "said         -58\n",
            "'            -67\n",
            "-           -162\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4jPbneK01n8"
      },
      "source": [
        "These differences suggest that fake news articles focused more on the actions of specific people (`trump`, `clinton`) and less on specific details (`tuesday`, `financial`).\n",
        "\n",
        "However, these results could be due to longer articles that allowed e.g. real news writers to cover more details. How do we control for length?\n",
        "\n",
        "Let's compute the normalized frequency for fake news and real news articles, to identify words that occurred more often than expected in one genre of article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alao_GLP01n-"
      },
      "source": [
        "### Normalized frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhCEeC4L01oA"
      },
      "source": [
        "def compute_norm_frequency(text_data, tokenizer, stops, vocab):\n",
        "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
        "                         ngram_range=(1,1), vocabulary=vocab)\n",
        "    dtm = cv.fit_transform(text_data)\n",
        "    # normalize by column\n",
        "    word_norm_frequency = np.array(dtm.sum(axis=0) / dtm.sum(axis=0).sum())[0]\n",
        "    # store in format that is easy to manipulate\n",
        "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
        "    return word_norm_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIjzKq_w01oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7da827-0418-43ff-fcda-f0f42bf34000"
      },
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "stops = get_stop_words('en')\n",
        "fake_news_word_norm_frequency = compute_norm_frequency(fake_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
        "real_news_word_norm_frequency = compute_norm_frequency(real_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
        "## compute ratio: what words are used more often in fake news than real news?\n",
        "def compute_text_word_ratio(text_data_1, text_data_2):\n",
        "    text_word_ratio = text_data_1 / text_data_2\n",
        "    # drop non-occurring words\n",
        "    text_word_ratio = text_word_ratio[~np.isinf(text_word_ratio)]\n",
        "    text_word_ratio = text_word_ratio[~np.isnan(text_word_ratio)]\n",
        "    text_word_ratio = text_word_ratio[text_word_ratio != 0.]\n",
        "    text_word_ratio.sort_values(inplace=True, ascending=False)\n",
        "    return text_word_ratio\n",
        "fake_vs_real_news_word_frequency_ratio = compute_text_word_ratio(fake_news_word_norm_frequency, real_news_word_norm_frequency)\n",
        "# show words with highest/lowest ratio\n",
        "top_k = 20\n",
        "print('words that occurred in more fake news articles')\n",
        "print(fake_vs_real_news_word_frequency_ratio.head(top_k))\n",
        "print('words that occurred in more real news articles')\n",
        "print(fake_vs_real_news_word_frequency_ratio.tail(top_k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words that occurred in more fake news articles\n",
            "hillary      11.724431\n",
            "commented    10.886971\n",
            "needs        10.049512\n",
            "secret        9.212053\n",
            "caused        8.374593\n",
            "ai            8.374593\n",
            "provided      7.537134\n",
            "earth         6.699675\n",
            "begin         6.699675\n",
            "instead       6.699675\n",
            "attempt       5.862215\n",
            "release       5.862215\n",
            "success       5.862215\n",
            "stein         5.862215\n",
            "lack          5.862215\n",
            "charges       5.024756\n",
            "tennis        5.024756\n",
            "met           5.024756\n",
            "phone         5.024756\n",
            "groups        5.024756\n",
            "dtype: float64\n",
            "words that occurred in more real news articles\n",
            "anniversary    0.167492\n",
            "customer       0.167492\n",
            "missing        0.167492\n",
            "saw            0.167492\n",
            "value          0.167492\n",
            "jersey         0.167492\n",
            "providers      0.167492\n",
            "potentially    0.167492\n",
            "growing        0.139577\n",
            "indian         0.139577\n",
            "story          0.139577\n",
            "drawn          0.139577\n",
            "january        0.139577\n",
            "february       0.139577\n",
            "vehicle        0.119637\n",
            "brady          0.119637\n",
            "40             0.119637\n",
            "18             0.119637\n",
            "amateur        0.119637\n",
            "birth          0.104682\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7SfLweg01oB"
      },
      "source": [
        "OK! We see that fake news consistently focuses on `hillary` (e.g. her email case) s well as potential conspiracy theories (`secret`, `ai`). In contrast, real news focuses on concrete time details (`january`, `40`) and provides some words to \"hedge\" their claims (`potentially`, `story`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ZfLp2k01oB"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8VNdZj601oC"
      },
      "source": [
        "What if we want to identify words that occur frequency in just a few documents? E.g. some fake news stories may disproportionately use rare but inflammatory words.\n",
        "\n",
        "Let's try TF-IDF, which normalizes term frequency by the inverse document frequency:\n",
        "\n",
        "$$\\text{tf-idf(word)} = \\frac{\\text{freq(word)}}{\\text{document-freq(word)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F204Wm6Y01oC"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def compute_non_zero_mean(data):\n",
        "    non_zero_data = data[data != 0.]\n",
        "    non_zero_mean = non_zero_data.mean()\n",
        "    return non_zero_mean\n",
        "def compute_tfidf(text_data, tokenizer, stops, vocab):\n",
        "    tfidf_vec = TfidfVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops, vocabulary=vocab)\n",
        "    text_tfidf_matrix = tfidf_vec.fit_transform(text_data).toarray()\n",
        "    # compute mean over non-zero TF-IDF values\n",
        "    text_tfidf_score = np.apply_along_axis(lambda x: compute_non_zero_mean(x), 0, text_tfidf_matrix)\n",
        "    text_tfidf_score = pd.Series(text_tfidf_score, index=vocab)\n",
        "    return text_tfidf_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2CwsjVH01oC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d596f82-8976-4371-a442-7ec27c430b5d"
      },
      "source": [
        "fake_news_tfidf = compute_tfidf(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
        "real_news_tfidf = compute_tfidf(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
        "fake_vs_real_news_word_tfidf_ratio = fake_news_tfidf / real_news_tfidf\n",
        "fake_vs_real_news_word_tfidf_ratio.dropna(inplace=True)\n",
        "fake_vs_real_news_word_tfidf_ratio.sort_values(inplace=True, ascending=False)\n",
        "top_k = 20\n",
        "print('words with higher TF-IDF scores in fake news')\n",
        "display(fake_vs_real_news_word_tfidf_ratio.head(top_k))\n",
        "print('words with higher TF-IDF scores in real news')\n",
        "display(fake_vs_real_news_word_tfidf_ratio.tail(top_k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: Mean of empty slice.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: Mean of empty slice.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "words with higher TF-IDF scores in fake news\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "steel         4.280091\n",
              "retailers     4.089067\n",
              "tourists      3.949276\n",
              "friendship    3.927483\n",
              "morgan        3.752379\n",
              "bruno         3.728585\n",
              "gas           3.509625\n",
              "saudi         3.392921\n",
              "arnold        3.364493\n",
              "privacy       3.356762\n",
              "sacrifice     3.214699\n",
              "emoji         3.112207\n",
              "duncan        3.080392\n",
              "michelle      3.053198\n",
              "ebony         3.014936\n",
              "qatar         2.980143\n",
              "fees          2.926611\n",
              "ai            2.926181\n",
              "wawrinka      2.924895\n",
              "kyrgios       2.892984\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "words with higher TF-IDF scores in real news\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "virtual           0.331943\n",
              "comfortable       0.330759\n",
              "saran             0.312199\n",
              "hacking           0.309510\n",
              "junco             0.308395\n",
              "farah             0.303634\n",
              "iphones           0.300928\n",
              "putin             0.297846\n",
              "engines           0.292603\n",
              "fisher            0.280006\n",
              "alcohol           0.279075\n",
              "absurdity         0.277041\n",
              "suddenly          0.272967\n",
              "pizzagate         0.270884\n",
              "punk              0.265890\n",
              "authentication    0.264018\n",
              "factor            0.264018\n",
              "graduates         0.254588\n",
              "tempe             0.239919\n",
              "investigators     0.221240\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfvkhsnA01oC"
      },
      "source": [
        "This method succeeds in identifying fairly rare words that characterize real and fake news.\n",
        "\n",
        "For fake news, we see that words with higher TF-IDF scores include those related to business transactions (`retailers`, `gas`) and Middle Eastern countries (`saudi`, `qatar`).\n",
        "\n",
        "For real news, the words with higher TF-IDF scores include words that directly address conspiracies (`pizzagate`, `investigators`, `authentication`) and words that speculate on the veracity of claims (`absurdity`, `suddenly`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIVlE6u601oD"
      },
      "source": [
        "### Exploration\n",
        "\n",
        "Now it's time for you to explore the data a little more with word frequency modeling!\n",
        "\n",
        "Some thoughts:\n",
        "\n",
        "- The original data are organized by topic. What are the words that characterize real/fake news in each topic?\n",
        "- Changing the vocabulary size could identify more rare words (e.g. lowering `min_df` threshold in `CountVectorizer`). What happens if you include more words in the vocabulary?\n",
        "- Up until now we have focused more strongly on single words (unigrams). What if we include phrases (changing `ngram_range` in the `CountVectorizer`)? Will we see more examples of conspiracy theories being highlighted by the real news?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGt8P-RE01oD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "969aea5b-f41a-483c-f2c5-893161de617e"
      },
      "source": [
        "## example: test different n-gram range\n",
        "## generate new vocabulary\n",
        "tokenizer = WordPunctTokenizer()\n",
        "en_stops = get_stop_words('en')\n",
        "def compute_word_freq_custom(text_data, custom_cv, vocab):\n",
        "    text_dtm = custom_cv.transform(text_data)\n",
        "    word_norm_frequency = np.array(text_dtm.sum(axis=0) / text_dtm.sum(axis=0).sum())[0]\n",
        "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
        "    return word_norm_frequency\n",
        "# create custom vectorizer for bigrams\n",
        "bigram_cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
        "                            tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
        "                            ngram_range=(2,2))\n",
        "# get vocab for all data\n",
        "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
        "combined_txt_dtm = bigram_cv.fit_transform(combined_txt)\n",
        "sorted_bigram_vocab = list(sorted(bigram_cv.vocabulary_.keys(), key=bigram_cv.vocabulary_.get))\n",
        "## compute frequency ratio for bigrams\n",
        "fake_news_bigram_frequency = compute_word_freq_custom(fake_news_data.loc[:, 'text'].values, bigram_cv, sorted_bigram_vocab)\n",
        "real_news_bigram_frequency = compute_word_freq_custom(real_news_data.loc[:, 'text'].values, bigram_cv, sorted_bigram_vocab)\n",
        "fake_vs_real_news_bigram_word_frequency_ratio = compute_text_word_ratio(fake_news_bigram_frequency, real_news_bigram_frequency)\n",
        "fake_vs_real_news_bigram_word_frequency_ratio.sort_values(inplace=True, ascending=False)\n",
        "top_k = 20\n",
        "print('top unigrams/bigrams that occur more often in fake news data')\n",
        "display(fake_vs_real_news_bigram_word_frequency_ratio.head(top_k))\n",
        "print('top unigrams/bigrams that occur more often in real news data')\n",
        "display(fake_vs_real_news_bigram_word_frequency_ratio.tail(top_k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top unigrams/bigrams that occur more often in fake news data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "hillary clinton     10.837533\n",
              ", \"                  7.711322\n",
              "however ,            7.502908\n",
              "trump .              7.502908\n",
              ", many               6.669251\n",
              ", one                5.835595\n",
              "game ,               5.001938\n",
              "president donald     5.001938\n",
              "couldn '             5.001938\n",
              "first lady           5.001938\n",
              "white house          4.724053\n",
              "donald trump         4.335013\n",
              "monday .             4.168282\n",
              "night ,              4.168282\n",
              "now ,                4.168282\n",
              "trump tower          4.168282\n",
              "\" just               4.168282\n",
              ". new                3.751454\n",
              ", wanted             3.334626\n",
              "supreme court        3.334626\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "top unigrams/bigrams that occur more often in real news data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "economy .        0.208414\n",
              "performance -    0.208414\n",
              "2 .              0.208414\n",
              "well -           0.208414\n",
              ". k              0.208414\n",
              ". 4              0.208414\n",
              "2016 .           0.208414\n",
              ". still          0.208414\n",
              "s really         0.208414\n",
              ", adding         0.208414\n",
              "science ,        0.208414\n",
              "k .              0.208414\n",
              "- year           0.189467\n",
              "year -           0.185257\n",
              "- old            0.175507\n",
              "middle east      0.166731\n",
              "indian wells     0.166731\n",
              ". report         0.166731\n",
              "world number     0.166731\n",
              "1 .              0.083366\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtDqu0kcooIH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}