{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency\n",
    "\n",
    "First step in exploration: which words occur more frequently in one data set versus another?\n",
    "\n",
    "1. [Load data](#Load-data)\n",
    "2. [Raw frequency](#Raw-frequency)\n",
    "3. [Normalized frequency](#Normalized-frequency)\n",
    "4. [TF-IDF](#TF-IDF)\n",
    "5. [Exploration](#Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex Jones Vindicated in \"Pizzagate\" Controversy</td>\n",
       "      <td>\"Alex Jones, purveyor of the independent inves...</td>\n",
       "      <td>biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE BIG DATA CONSPIRACY</td>\n",
       "      <td>so that in the no so far future can institute ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California Surprisingly Lenient on Auto Emissi...</td>\n",
       "      <td>Setting Up Face-Off With Trump \"California's c...</td>\n",
       "      <td>biz</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mexicans Are Chomping at the Bit to Stop NAFTA...</td>\n",
       "      <td>Mexico has been unfairly gaining from NAFTA as...</td>\n",
       "      <td>biz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breaking News: Snapchat to purchase Twitter fo...</td>\n",
       "      <td>Yahoo and AOL could be extremely popular over ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Alex Jones Vindicated in \"Pizzagate\" Controversy   \n",
       "1                            THE BIG DATA CONSPIRACY   \n",
       "2  California Surprisingly Lenient on Auto Emissi...   \n",
       "3  Mexicans Are Chomping at the Bit to Stop NAFTA...   \n",
       "4  Breaking News: Snapchat to purchase Twitter fo...   \n",
       "\n",
       "                                                text topic  id  \n",
       "0  \"Alex Jones, purveyor of the independent inves...   biz   1  \n",
       "1  so that in the no so far future can institute ...   biz   2  \n",
       "2  Setting Up Face-Off With Trump \"California's c...   biz   3  \n",
       "3  Mexico has been unfairly gaining from NAFTA as...   biz   4  \n",
       "4  Yahoo and AOL could be extremely popular over ...   biz   5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex Jones Vindicated in \"Pizzagate\" Controversy</td>\n",
       "      <td>\"Alex Jones, purveyor of the independent inves...</td>\n",
       "      <td>biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE BIG DATA CONSPIRACY</td>\n",
       "      <td>so that in the no so far future can institute ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California Surprisingly Lenient on Auto Emissi...</td>\n",
       "      <td>Setting Up Face-Off With Trump \"California's c...</td>\n",
       "      <td>biz</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mexicans Are Chomping at the Bit to Stop NAFTA...</td>\n",
       "      <td>Mexico has been unfairly gaining from NAFTA as...</td>\n",
       "      <td>biz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breaking News: Snapchat to purchase Twitter fo...</td>\n",
       "      <td>Yahoo and AOL could be extremely popular over ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Alex Jones Vindicated in \"Pizzagate\" Controversy   \n",
       "1                            THE BIG DATA CONSPIRACY   \n",
       "2  California Surprisingly Lenient on Auto Emissi...   \n",
       "3  Mexicans Are Chomping at the Bit to Stop NAFTA...   \n",
       "4  Breaking News: Snapchat to purchase Twitter fo...   \n",
       "\n",
       "                                                text topic  id  \n",
       "0  \"Alex Jones, purveyor of the independent inves...   biz   1  \n",
       "1  so that in the no so far future can institute ...   biz   2  \n",
       "2  Setting Up Face-Off With Trump \"California's c...   biz   3  \n",
       "3  Mexico has been unfairly gaining from NAFTA as...   biz   4  \n",
       "4  Yahoo and AOL could be extremely popular over ...   biz   5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fake_news_data = pd.read_csv('data/fakeNewsDatasets/fake_news_small.tsv', sep='\\t', index_col=False)\n",
    "real_news_data = pd.read_csv('data/fakeNewsDatasets/real_news_small.tsv', sep='\\t', index_col=False)\n",
    "display(fake_news_data.head())\n",
    "display(real_news_data.head())\n",
    "print(fake_news_data.shape[0])\n",
    "# print(fake_news_data[5])\n",
    "# print(real_news_data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw frequency\n",
    "\n",
    "As a first step, let's look at raw word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                     tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                     ngram_range=(1,1))\n",
    "# get vocab for all data\n",
    "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
    "combined_txt_dtm = cv.fit_transform(combined_txt)\n",
    "sorted_vocab = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
    "# get separate DTM for each news data\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, tokenizer=tokenizer.tokenize, stop_words=en_stops, vocabulary=sorted_vocab)\n",
    "fake_news_dtm = cv.fit_transform(fake_news_data.loc[:, 'text'].values)\n",
    "real_news_dtm = cv.fit_transform(real_news_data.loc[:, 'text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'            384\n",
      "\"            343\n",
      "s            278\n",
      "-            182\n",
      "will         161\n",
      "trump        113\n",
      "said          90\n",
      ".\"            89\n",
      "new           86\n",
      "president     76\n",
      "t             61\n",
      "time          56\n",
      "can           54\n",
      "school        53\n",
      "one           53\n",
      "now           49\n",
      "many          48\n",
      "just          43\n",
      "years         43\n",
      "students      42\n",
      "dtype: int64\n",
      "'            384\n",
      "\"            343\n",
      "s            278\n",
      "-            182\n",
      "will         161\n",
      "trump        113\n",
      "said          90\n",
      ".\"            89\n",
      "new           86\n",
      "president     76\n",
      "t             61\n",
      "time          56\n",
      "can           54\n",
      "school        53\n",
      "one           53\n",
      "now           49\n",
      "many          48\n",
      "just          43\n",
      "years         43\n",
      "students      42\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## top words\n",
    "import numpy as np\n",
    "fake_news_dtm_top_words = pd.Series(np.array(fake_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
    "real_news_dtm_top_words = pd.Series(np.array(real_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
    "print(fake_news_dtm_top_words.head(20))\n",
    "print(real_news_dtm_top_words.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = biz\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'            41\n",
       "\"            34\n",
       "s            33\n",
       "will         28\n",
       "-            28\n",
       "uk           21\n",
       "said         19\n",
       "$            14\n",
       ".\"           13\n",
       "eu           13\n",
       "trump        13\n",
       "deal         13\n",
       "company      11\n",
       "many         10\n",
       "european     10\n",
       "companies    10\n",
       "now           9\n",
       "may           8\n",
       "last          8\n",
       "new           8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'            41\n",
       "\"            34\n",
       "s            33\n",
       "will         28\n",
       "-            28\n",
       "uk           21\n",
       "said         19\n",
       "$            14\n",
       ".\"           13\n",
       "eu           13\n",
       "trump        13\n",
       "deal         13\n",
       "company      11\n",
       "many         10\n",
       "european     10\n",
       "companies    10\n",
       "now           9\n",
       "may           8\n",
       "last          8\n",
       "new           8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = edu\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            53\n",
       "school       47\n",
       "'            45\n",
       "students     38\n",
       "s            31\n",
       "-            23\n",
       "will         22\n",
       "education    20\n",
       "trump        15\n",
       "president    12\n",
       "new          12\n",
       "children     11\n",
       "schools      10\n",
       "first        10\n",
       "parents      10\n",
       ".\"           10\n",
       "law          10\n",
       "said         10\n",
       "student      10\n",
       "time         10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            53\n",
       "school       47\n",
       "'            45\n",
       "students     38\n",
       "s            31\n",
       "-            23\n",
       "will         22\n",
       "education    20\n",
       "trump        15\n",
       "president    12\n",
       "new          12\n",
       "children     11\n",
       "schools      10\n",
       "first        10\n",
       "parents      10\n",
       ".\"           10\n",
       "law          10\n",
       "said         10\n",
       "student      10\n",
       "time         10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = entmt\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            106\n",
       "s             64\n",
       "-             31\n",
       "will          29\n",
       ".\"            25\n",
       "t             24\n",
       "one           17\n",
       "new           16\n",
       "time          16\n",
       "show          16\n",
       "said          14\n",
       "also          14\n",
       "fans          13\n",
       "way           12\n",
       "last          11\n",
       "now           11\n",
       "(             11\n",
       "just          11\n",
       "character     10\n",
       "actress       10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            106\n",
       "s             64\n",
       "-             31\n",
       "will          29\n",
       ".\"            25\n",
       "t             24\n",
       "one           17\n",
       "new           16\n",
       "time          16\n",
       "show          16\n",
       "said          14\n",
       "also          14\n",
       "fans          13\n",
       "way           12\n",
       "last          11\n",
       "now           11\n",
       "(             11\n",
       "just          11\n",
       "character     10\n",
       "actress       10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = polit\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"             59\n",
       "s             56\n",
       "president     50\n",
       "clinton       29\n",
       "-             25\n",
       "donald        22\n",
       "said          20\n",
       "white         16\n",
       "house         16\n",
       "washington    15\n",
       "will          14\n",
       ".\"            13\n",
       "t             11\n",
       "us            11\n",
       "cnn           11\n",
       "just          11\n",
       "obama         11\n",
       "great         11\n",
       "(             11\n",
       ")             11\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"             59\n",
       "s             56\n",
       "president     50\n",
       "clinton       29\n",
       "-             25\n",
       "donald        22\n",
       "said          20\n",
       "white         16\n",
       "house         16\n",
       "washington    15\n",
       "will          14\n",
       ".\"            13\n",
       "t             11\n",
       "us            11\n",
       "cnn           11\n",
       "just          11\n",
       "obama         11\n",
       "great         11\n",
       "(             11\n",
       ")             11\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = sports\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"         55\n",
       "s         51\n",
       "-         41\n",
       "will      26\n",
       "game      24\n",
       "team      23\n",
       ".\"        21\n",
       "said      18\n",
       "two       16\n",
       "one       14\n",
       "years     13\n",
       "year      13\n",
       "last      12\n",
       "time      11\n",
       "new       10\n",
       "brazil    10\n",
       "world     10\n",
       "race       9\n",
       "sports     9\n",
       "just       9\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"         55\n",
       "s         51\n",
       "-         41\n",
       "will      26\n",
       "game      24\n",
       "team      23\n",
       ".\"        21\n",
       "said      18\n",
       "two       16\n",
       "one       14\n",
       "years     13\n",
       "year      13\n",
       "last      12\n",
       "time      11\n",
       "new       10\n",
       "brazil    10\n",
       "world     10\n",
       "race       9\n",
       "sports     9\n",
       "just       9\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = tech\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           49\n",
       "s           43\n",
       "will        42\n",
       "\"           36\n",
       "new         34\n",
       "-           34\n",
       "can         16\n",
       "amazon      14\n",
       "google      13\n",
       "now         12\n",
       "many        11\n",
       "apple       10\n",
       "said         9\n",
       "(            9\n",
       "t            9\n",
       "devices      9\n",
       "time         9\n",
       "world        9\n",
       "research     8\n",
       "youtube      8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           49\n",
       "s           43\n",
       "will        42\n",
       "\"           36\n",
       "new         34\n",
       "-           34\n",
       "can         16\n",
       "amazon      14\n",
       "google      13\n",
       "now         12\n",
       "many        11\n",
       "apple       10\n",
       "said         9\n",
       "(            9\n",
       "t            9\n",
       "devices      9\n",
       "time         9\n",
       "world        9\n",
       "research     8\n",
       "youtube      8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# per-topic\n",
    "article_topics = fake_news_data.loc[:, 'topic'].unique()\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "top_k = 20\n",
    "for topic_i in article_topics:\n",
    "    print(f'topic = {topic_i}')\n",
    "    fake_news_data_i = fake_news_data[fake_news_data.loc[:, 'topic']==topic_i]\n",
    "    real_news_data_i = real_news_data[real_news_data.loc[:, 'topic']==topic_i]\n",
    "    # get vocab, compute counts, etc.\n",
    "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                         ngram_range=(1,1))\n",
    "    combined_txt_i = fake_news_data_i.loc[:, 'text'].append(real_news_data_i.loc[:, 'text'])\n",
    "    combined_txt_dtm_i = cv.fit_transform(combined_txt_i)\n",
    "    sorted_vocab_i = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
    "    # get separate DTM for each news data\n",
    "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                         ngram_range=(1,1), vocabulary=sorted_vocab_i)\n",
    "    fake_news_dtm_i = cv.fit_transform(fake_news_data_i.loc[:, 'text'].values)\n",
    "    real_news_dtm_i = cv.fit_transform(real_news_data_i.loc[:, 'text'].values)\n",
    "    # get top counts\n",
    "    fake_news_dtm_top_words_i = pd.Series(np.array(fake_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
    "    real_news_dtm_top_words_i = pd.Series(np.array(real_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
    "    print('top words for fake news articles')\n",
    "    display(fake_news_dtm_top_words_i)\n",
    "    print('top words for real news articles')\n",
    "    display(real_news_dtm_top_words_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency(text_data, tokenizer, stops, vocab):\n",
    "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
    "                         ngram_range=(1,1), vocabulary=vocab)\n",
    "    dtm = cv.fit_transform(text_data)\n",
    "    word_frequency = np.array(dtm.sum(axis=0))[0]\n",
    "    word_frequency = pd.Series(word_frequency, index=vocab)\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words that occurred in more fake news articles\n",
      "���            0\n",
      "eu             0\n",
      "espen          0\n",
      "esporte        0\n",
      "essentially    0\n",
      "estate         0\n",
      "estimated      0\n",
      "et             0\n",
      "ethnically     0\n",
      "ethnicity      0\n",
      "euro           0\n",
      "escorted       0\n",
      "euronext       0\n",
      "europe         0\n",
      "european       0\n",
      "eurpoe         0\n",
      "evasions       0\n",
      "eve            0\n",
      "even           0\n",
      "evening        0\n",
      "dtype: int64\n",
      "words that occurred in more real news articles\n",
      "player      0\n",
      "played      0\n",
      "placing     0\n",
      "plagued     0\n",
      "plaguing    0\n",
      "plainly     0\n",
      "plan        0\n",
      "planes      0\n",
      "planet      0\n",
      "planned     0\n",
      "planning    0\n",
      "plans       0\n",
      "plant       0\n",
      "planted     0\n",
      "plants      0\n",
      "plated      0\n",
      "platform    0\n",
      "platonic    0\n",
      "play        0\n",
      "!           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fake_news_text = fake_news_data.loc[:, 'text'].values\n",
    "real_news_text = real_news_data.loc[:, 'text'].values\n",
    "fake_news_word_frequency = compute_frequency(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "real_news_word_frequency = compute_frequency(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "# compute difference\n",
    "fake_vs_real_news_word_frequency_diff = fake_news_word_frequency - real_news_word_frequency\n",
    "fake_vs_real_news_word_frequency_diff.sort_values(inplace=True, ascending=False)\n",
    "# show words with highest/lowest difference\n",
    "top_k = 20\n",
    "print('words that occurred in more fake news articles')\n",
    "print(fake_vs_real_news_word_frequency_diff.head(top_k))\n",
    "print('words that occurred in more real news articles')\n",
    "print(fake_vs_real_news_word_frequency_diff.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These differences suggest that fake news articles focused more on the actions of specific people (`trump`, `clinton`) and less on specific details (`tuesday`, `financial`).\n",
    "\n",
    "However, these results could be due to longer articles that allowed e.g. real news writers to cover more details. How do we control for length?\n",
    "\n",
    "Let's compute the normalized frequency for fake news and real news articles, to identify words that occurred more often than expected in one genre of article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_frequency(text_data, tokenizer, stops, vocab):\n",
    "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
    "                         ngram_range=(1,1), vocabulary=vocab)\n",
    "    dtm = cv.fit_transform(text_data)\n",
    "    # normalize by column\n",
    "    word_norm_frequency = np.array(dtm.sum(axis=0) / dtm.sum(axis=0).sum())[0]\n",
    "    # store in format that is easy to manipulate\n",
    "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
    "    return word_norm_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stops = get_stop_words('en')\n",
    "fake_news_word_norm_frequency = compute_norm_frequency(fake_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
    "real_news_word_norm_frequency = compute_norm_frequency(real_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
    "## compute ratio: what words are used more often in fake news than real news?\n",
    "def compute_text_word_ratio(text_data_1, text_data_2):\n",
    "    text_word_ratio = text_data_1 / text_data_2\n",
    "    # drop non-occurring words\n",
    "    text_word_ratio = text_word_ratio[~np.isinf(text_word_ratio)]\n",
    "    text_word_ratio = text_word_ratio[~np.isnan(text_word_ratio)]\n",
    "    text_word_ratio = text_word_ratio[text_word_ratio != 0.]\n",
    "    text_word_ratio.sort_values(inplace=True, ascending=False)\n",
    "    return text_word_ratio\n",
    "fake_vs_real_news_word_frequency_ratio = compute_text_word_ratio(fake_news_word_norm_frequency, real_news_word_norm_frequency)\n",
    "# show words with highest/lowest ratio\n",
    "top_k = 20\n",
    "print('words that occurred in more fake news articles')\n",
    "print(fake_real_news_word_frequency_ratio.head(top_k))\n",
    "print('words that occurred in more real news articles')\n",
    "print(fake_real_news_word_frequency_ratio.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We see that fake news consistently focuses on `hillary` (e.g. her email case) s well as potential conspiracy theories (`secret`, `ai`). In contrast, real news focuses on concrete time details (`january`, `40`) and provides some words to \"hedge\" their claims (`potentially`, `story`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to identify words that occur frequency in just a few documents? E.g. some fake news stories may disproportionately use rare but inflammatory words.\n",
    "\n",
    "Let's try TF-IDF, which normalizes term frequency by the inverse document frequency:\n",
    "\n",
    "$$\\text{tf-idf(word)} = \\frac{\\text{freq(word)}}{\\text{document-freq(word)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def compute_non_zero_mean(data):\n",
    "    non_zero_data = data[data != 0.]\n",
    "    non_zero_mean = non_zero_data.mean()\n",
    "    return non_zero_mean\n",
    "def compute_tfidf(text_data, tokenizer, stops, vocab):\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops, vocabulary=vocab)\n",
    "    text_tfidf_matrix = tfidf_vec.fit_transform(text_data).toarray()\n",
    "    # compute mean over non-zero TF-IDF values\n",
    "    text_tfidf_score = np.apply_along_axis(lambda x: compute_non_zero_mean(x), 0, text_tfidf_matrix)\n",
    "    text_tfidf_score = pd.Series(text_tfidf_score, index=vocab)\n",
    "    return text_tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_tfidf = compute_tfidf(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "real_news_tfidf = compute_tfidf(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "fake_vs_real_news_word_tfidf_ratio = fake_news_tfidf / real_news_tfidf\n",
    "fake_vs_real_news_word_tfidf_ratio.dropna(inplace=True)\n",
    "fake_vs_real_news_word_tfidf_ratio.sort_values(inplace=True, ascending=False)\n",
    "top_k = 20\n",
    "print('words with higher TF-IDF scores in fake news')\n",
    "display(fake_vs_real_news_word_tfidf_ratio.head(top_k))\n",
    "print('words with higher TF-IDF scores in real news')\n",
    "display(fake_vs_real_news_word_tfidf_ratio.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method succeeds in identifying fairly rare words that characterize real and fake news.\n",
    "\n",
    "For fake news, we see that words with higher TF-IDF scores include those related to business transactions (`retailers`, `gas`) and Middle Eastern countries (`saudi`, `qatar`).\n",
    "\n",
    "For real news, the words with higher TF-IDF scores include words that directly address conspiracies (`pizzagate`, `investigators`, `authentication`) and words that speculate on the veracity of claims (`absurdity`, `suddenly`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Now it's time for you to explore the data a little more with word frequency modeling!\n",
    "\n",
    "Some thoughts:\n",
    "\n",
    "- The original data are organized by topic. What are the words that characterize real/fake news in each topic?\n",
    "- Changing the vocabulary size could identify more rare words (e.g. lowering `min_df` threshold in `CountVectorizer`). What happens if you include more words in the vocabulary?\n",
    "- Up until now we have focused more strongly on single words (unigrams). What if we include phrases (changing `ngram_range` in the `CountVectorizer`)? Will we see more examples of conspiracy theories being highlighted by the real news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example: test different n-gram range\n",
    "## generate new vocabulary\n",
    "tokenizer = WordPunctTokenizer()\n",
    "en_stops = get_stop_words('en')\n",
    "def compute_word_freq_custom(text_data, custom_cv, vocab):\n",
    "    text_dtm = custom_cv.transform(text_data)\n",
    "    word_norm_frequency = np.array(text_dtm.sum(axis=0) / text_dtm.sum(axis=0).sum())[0]\n",
    "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
    "    return word_norm_frequency\n",
    "# create custom vectorizer for bigrams\n",
    "bigram_cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                            tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                            ngram_range=(2,2))\n",
    "# get vocab for all data\n",
    "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
    "combined_txt_dtm = bigram_cv.fit_transform(combined_txt)\n",
    "sorted_bigram_vocab = list(sorted(bigram_cv.vocabulary_.keys(), key=bigram_cv.vocabulary_.get))\n",
    "## compute frequency ratio for bigrams\n",
    "fake_news_bigram_frequency = compute_word_freq_custom(fake_news_data.loc[:, 'text'].values, bigram_cv, sorted_bigram_vocab)\n",
    "fake_vs_real_news_bigram_word_frequency_ratio = compute_text_word_ratio(fake_news_bigram_frequency, real_news_bigram_frequency)\n",
    "fake_vs_real_news_bigram_word_frequency_ratio.sort_values(inplace=True, ascending=False)\n",
    "top_k = 20\n",
    "print('top unigrams/bigrams that occur more often in fake news data')\n",
    "display(fake_vs_real_news_bigram_word_frequency_ratio.head(top_k))\n",
    "print('top unigrams/bigrams that occur more often in real news data')\n",
    "display(fake_vs_real_news_bigram_word_frequency_ratio.tail(top_k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CORE_tutorial] *",
   "language": "python",
   "name": "conda-env-CORE_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
